{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.early_stopping import EarlyStopping\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from model.ha import HierarchicalAttPredictor\n",
    "from sklearn.metrics import classification_report\n",
    "# from data.evaluate import load_dev_labels, get_metrics\n",
    "import pickle as pkl\n",
    "import sys\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from copy import deepcopy\n",
    "import argparse\n",
    "import random\n",
    "from utils.focalloss import FocalLoss\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
    "# from utils.tweet_processor import processing_pipeline\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from module.preprocessor import EnglishPreProcessor\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from config.basic_config import configs as config\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testnp = np.array([[1, 2], [3, 4]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teststart = []\n",
    "teststart.append(np.asarray([1, 2]))\n",
    "teststart.append(np.asarray([3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(teststart).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "teststart = []\n",
    "teststart.append([1, 2])\n",
    "teststart.append([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(teststart).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "testnp=[[1,2],[1,2,3],[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "testnp=np.array(testnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 336/336 [00:00<00:00, 834885.16B/s]\n",
      "100%|██████████| 374434792/374434792 [00:06<00:00, 59699433.11B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Elmo(\n",
       "  (_elmo_lstm): _ElmoBiLm(\n",
       "    (_token_embedder): _ElmoCharacterEncoder(\n",
       "      (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
       "      (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))\n",
       "      (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))\n",
       "      (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))\n",
       "      (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))\n",
       "      (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))\n",
       "      (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))\n",
       "      (_highways): Highway(\n",
       "        (_layers): ModuleList(\n",
       "          (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "          (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (_projection): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    )\n",
       "    (_elmo_lstm): ElmoLstm(\n",
       "      (forward_layer_0): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "      (backward_layer_0): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "      (forward_layer_1): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "      (backward_layer_1): LstmCellWithProjection(\n",
       "        (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
       "        (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
       "        (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_dropout): Dropout(p=0)\n",
       "  (scalar_mix_0): ScalarMix(\n",
       "    (scalar_parameters): ParameterList(\n",
       "        (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "        (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "        (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "    )\n",
       "  )\n",
       "  (scalar_mix_1): ScalarMix(\n",
       "    (scalar_parameters): ParameterList(\n",
       "        (0): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "        (1): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "        (2): Parameter containing: [torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0).cuda()\n",
    "elmo.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = EnglishPreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc pipeline not working, have to remove it\n",
    "def load_data_context(data_path='/data/SuperMod/test_data.txt', is_train=True):\n",
    "\n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    if len(df.columns) > 4:\n",
    "        data_list = df.comment_text.tolist()\n",
    "        target_list = df.toxic.tolist()   \n",
    "\n",
    "    else:\n",
    "        data_list = df.comment_text.tolist()\n",
    "        target_list = df.toxicity.tolist()\n",
    "\n",
    "    clean_sent_list = [sent_tokenize(processing_pipeline(str(email))) for email in data_list]\n",
    "\n",
    "    if is_train:\n",
    "        return clean_sent_list, target_list\n",
    "    else:\n",
    "        return clean_sent_list\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/ToxicityDataOld/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andy          inline attachment follows          from : < email > to : zipper , andy < / o = enron / ou = na / cn = recipients / cn = azipper > date : friday , date time gmt subject : ha ha ha you stupid , arrogant fuck                                                    do you yahoo ?'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(train[4][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'god'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(.+?)\\1+', r'\\1', 'preprocessor(train[4][2])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import create_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(vocab_file='/root/projects/SuperMod/SentimentDetectionNLP/bertmodel/pybert/model/pretrain/uncased_L-12_H-768_A-12/vocab.txt',\n",
    "                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, label = load_data_context(data_path='/data/SuperMod/training_data_supermod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t, label_t = load_data_context(data_path='/data/ToxicityDataOld/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v, label_v = load_data_context(data_path='/data/ToxicityDataOld/test_with_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie = pd.read_csv(\"/data/SuperMod/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.columns = [\"comment_text\" , \"toxicity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie.to_csv(\"/data/SuperMod/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       comment_text  toxicity\n",
       "0           0  One of the other reviewers has mentioned that ...  positive\n",
       "1           1  A wonderful little production. <br /><br />The...  positive\n",
       "2           2  I thought this was a wonderful way to spend ti...  positive\n",
       "3           3  Basically there's a family where a little boy ...  negative\n",
       "4           4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m, label_m = load_data_context(data_path=\"/data/SuperMod/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 24205 words\n",
      "filling vocab to 20003\n"
     ]
    }
   ],
   "source": [
    "word2id, id2word, num_of_vocab = create_data.build_vocab([train_t, train_v, train, train_m], 20000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/SuperMod/word2id.pkl', 'wb') as w:\n",
    "    pkl.dump(word2id, w, pkl.HIGHEST_PROTOCOL)\n",
    "with open('/data/SuperMod/id2word.pkl', 'wb') as i:\n",
    "    pkl.dump(id2word, i, pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.9241770608646722,\n",
       "  0.5822163238221633,\n",
       "  0.7203612479474548,\n",
       "  0.6439633027522935],\n",
       " 1: [0.9173934790084092,\n",
       "  0.5460263007432818,\n",
       "  0.784072249589491,\n",
       "  0.6437478934951129],\n",
       " 2: [0.918643908843665,\n",
       "  0.5511738175089627,\n",
       "  0.7825944170771757,\n",
       "  0.6468073556354753],\n",
       " 3: [0.9190971896589453,\n",
       "  0.5530285449060107,\n",
       "  0.7825944170771757,\n",
       "  0.6480826760946424]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data/SuperMod/result.pkl', 'rb') as w:\n",
    "    old_result = pickle.load(w)\n",
    "\n",
    "old_result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>here is the latest physical tab template.  row...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What's up in the hood?  It sounds like you guy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Yo Dr. Ake -\\r\\n\\r\\nI WENT AND GOT MEASURED LA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>this request has been pending your approval fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yes, Elvis is still alive and reading these pu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       comment_text  toxicity\n",
       "0   0  here is the latest physical tab template.  row...         0\n",
       "1   1  What's up in the hood?  It sounds like you guy...         1\n",
       "2   2  Yo Dr. Ake -\\r\\n\\r\\nI WENT AND GOT MEASURED LA...         1\n",
       "3   3  this request has been pending your approval fo...         0\n",
       "4   4  Yes, Elvis is still alive and reading these pu...         1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s = pd.read_csv('/data/SuperMod/training_data_supermod.csv')\n",
    "df_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_s, label_s = load_data_context(data_path='/data/SuperMod/training_data_supermod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.match(\".*[a-zA-Z]+.*\", '.'):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(sent_text):\n",
    "    \"\"\"\n",
    "    This function detects if a line should be removed (all empty or no words)\n",
    "    And returns a cleaned sentences without duplicated tokens\n",
    "    \"\"\"\n",
    "    to_keep = False\n",
    "    if re.match(\".*[a-zA-Z]+.*\", sent_text):\n",
    "        to_keep = True\n",
    "    remove_dup = re.sub(r'(.+?)\\1+', r'\\1',processing_pipeline(sent_text))\n",
    "    return to_keep, remove_dup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_train_data (email_text):\n",
    "    \n",
    "    tmp = []\n",
    "\n",
    "    for sents in email_text:\n",
    "        keep, sents = clean_sentences(sents)\n",
    "        if keep:                \n",
    "            tmp.append(tokenizer.tokenize(sents))\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = [get_clean_train_data(email) for email in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_t = [get_clean_train_data(email) for email in train_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_train_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-292ca8e947ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_train_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_train_t' is not defined"
     ]
    }
   ],
   "source": [
    "clean_train_t[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len = []\n",
    "sent_num = []\n",
    "\n",
    "for i in clean_train:\n",
    "    sent_num.append(len(i))\n",
    "#     if len(i) > 50:\n",
    "#         print(i)\n",
    "    for j in i:\n",
    "        sent_len.append(len(j))\n",
    "#         if len(j) > 50:\n",
    "#             print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len_t = []\n",
    "sent_num_t = []\n",
    "\n",
    "for i in clean_train_t:\n",
    "    sent_num_t.append(len(i))\n",
    "#     if len(i) > 50:\n",
    "#         print(i)\n",
    "    for j in i:\n",
    "        sent_len_t.append(len(j))\n",
    "#         if len(j) > 50:\n",
    "#             print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9d250437b8>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9d0bd60320>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(sent_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.916873449131515\n",
      "17.765805692126584\n",
      "4.768942978360729\n",
      "19.15463248290045\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sent_num))\n",
    "print(np.mean(sent_len))\n",
    "print(np.mean(sent_num_t))\n",
    "print(np.mean(sent_len_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "12.0\n",
      "3.0\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "print(np.median(sent_num))\n",
    "print(np.median(sent_len))\n",
    "print(np.median(sent_num_t))\n",
    "print(np.median(sent_len_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(preprocessor(train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = EnglishPreProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' forwarded by dawn c kenne/hou/ect on 03/06/2001 10:38 am ---------------------------heightshappyataol.com on 03/06/2001 09:35:01 am'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' forwarded by dawn c kenne/hou/ect on 03/06/2001 10:38 am ---------------------------heightshappyataol.com on 03/06/2001 09:35:01 am']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['- - - - - - - - - - - - - - - - - - - - - - forwarded by dawn c kenne / hou / ect on date time am - - - - - - - - - - - - - - - - - - - - - - - - - - - email on date time to : email , email , email , email , email , email ( laura romaine ) , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email , email .',\n",
    " 'us , email , email cc : subject : things to say when you are at a loss for words .',\n",
    " 'repeated number ?',\n",
    " '.',\n",
    " 'repeated i can see your point , but i still think you are full of shit .',\n",
    " 'number ?',\n",
    " '.',\n",
    " \"repeated i do not know what your problem is , but i will bet it ' s hard to pronounce .\",\n",
    " 'number ?',\n",
    " '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forward',\n",
       " '##ed',\n",
       " 'by',\n",
       " 'dawn',\n",
       " 'c',\n",
       " 'ken',\n",
       " '##e',\n",
       " '/',\n",
       " 'ho',\n",
       " '##u',\n",
       " '/',\n",
       " 'ec',\n",
       " '##t',\n",
       " 'on',\n",
       " 'date',\n",
       " 'time',\n",
       " 'am',\n",
       " 'email',\n",
       " 'on',\n",
       " 'date',\n",
       " 'time',\n",
       " 'to',\n",
       " ':',\n",
       " 'email',\n",
       " ',',\n",
       " 'email',\n",
       " '(',\n",
       " 'laura',\n",
       " 'roma',\n",
       " '##ine',\n",
       " ')',\n",
       " ',',\n",
       " 'email',\n",
       " '.']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(re.sub(r'(.+?)\\1+', r'\\1', preprocessor(train[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAI_PAD_LEN = 3\n",
    "EMOJ_SENT_PAD_LEN = SENT_PAD_LEN = 10\n",
    "SENT_EMB_DIM = 300\n",
    "\n",
    "NUM_OF_FOLD = 9\n",
    "learning_rate = 5e-4\n",
    "MAX_EPOCH = 2\n",
    "\n",
    "FILL_VOCAB = True\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "SENT_HIDDEN_SIZE = 1500\n",
    "CLIP = 0.888\n",
    "EARLY_STOP_PATIENCE = 1\n",
    "LAMBDA1 = 0\n",
    "LAMBDA2 = 0\n",
    "FLAT = 1\n",
    "VOCAB_SIZE = 100\n",
    "\n",
    "gamma = 0.2\n",
    "loss = 'ce'\n",
    "w = 1\n",
    "focal = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(clean_sent_lists, vocab_size, fill_vocab=False):\n",
    "    \"\"\"\n",
    "    get all the words from the list, and sort them by count\n",
    "    Then convert them into ids.\n",
    "    This is simply word split, can improve with other tokenizer\n",
    "    clean_sent_lists is a list of list of tokenized emails\n",
    "    \"\"\"\n",
    "    word_count = {}\n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    \n",
    "    data_list_list = []\n",
    "    \n",
    "    for data_list in clean_sent_lists:\n",
    "        data_list_list.extend(data_list)\n",
    "    \n",
    "    for emails in data_list_list:\n",
    "        for sentence in emails:\n",
    "            keep, sents = clean_sentences(sentence)\n",
    "            if keep:\n",
    "                for word in tokenizer.tokenize(sents):\n",
    "                    if word in word_count:\n",
    "                        word_count[word] += 1\n",
    "                    else:\n",
    "                        word_count[word] = 1 \n",
    "\n",
    "    word_list = [x for x, _ in sorted(word_count.items(), key=lambda v: v[1], reverse=True)]\n",
    "    print('found', len(word_count), 'words')\n",
    "\n",
    "    if len(word_count) < vocab_size:\n",
    "        raise Exception('Vocab less than requested!!!')\n",
    "\n",
    "    # add <pad> first\n",
    "    word2id['<pad>'] = 0\n",
    "    id2word[0] = '<pad>'\n",
    "\n",
    "    word2id['<unk>'] = 1\n",
    "    id2word[1] = '<unk>'\n",
    "    word2id['<empty>'] = 2\n",
    "    id2word[2] = '<empty>'\n",
    "\n",
    "    n = len(word2id)\n",
    "    if not fill_vocab:\n",
    "        word_list = word_list[:vocab_size - n]\n",
    "\n",
    "    for word in word_list:\n",
    "        word2id[word] = n\n",
    "        id2word[n] = word\n",
    "        n += 1\n",
    "\n",
    "    if fill_vocab:\n",
    "        print('filling vocab to', len(id2word))\n",
    "        return word2id, id2word, len(id2word)\n",
    "    return word2id, id2word, len(word2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2id, id2word, length = build_vocab([train], 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing using dictionary from /data/torchMoji/model/vocabulary.json\n"
     ]
    }
   ],
   "source": [
    "from module import create_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build vocab for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 24057 words\n",
      "filling vocab to 24060\n"
     ]
    }
   ],
   "source": [
    "##Buid vocab to be saved to file\n",
    "word2id, id2word, num_of_vocab = build_vocab([train_t, train_v, train_s], 5000, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/SuperMod/word2id.pkl', 'wb') as w:\n",
    "    pickle.dump(word2id, w, pickle.HIGHEST_PROTOCOL)\n",
    "with open('/data/SuperMod/id2word.pkl', 'wb') as i:\n",
    "    pickle.dump(id2word, i, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('/data/SuperMod/my_file.pkl', 'rb') as f:\n",
    "#     x = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataSet(Dataset):\n",
    "    def __init__(self, data_list, target_list, emai_pad_len, sent_pad_len, word2id,  use_unk=False):\n",
    "\n",
    "        self.sent_pad_len = sent_pad_len\n",
    "        self.emai_pad_len = emai_pad_len\n",
    "        self.word2id = word2id\n",
    "        self.pad_int = word2id['<pad>']\n",
    "\n",
    "        self.use_unk = use_unk\n",
    "\n",
    "        # internal data\n",
    "        # only have one input, not 3\n",
    "        self.a = []\n",
    "        self.a_len = []\n",
    "        self.emoji_a = []\n",
    "        \n",
    "        # e_c is the label of emotions\n",
    "        # for our project, the label is already 0, 1\n",
    "        # so remove binary and label translation\n",
    "        self.e_c = []\n",
    "#         self.e_c_binary = []\n",
    "#         self.e_c_emo = []\n",
    "        self.num_empty_lines = 0\n",
    "\n",
    "        self.weights = []\n",
    "        # prepare dataset\n",
    "        self.read_data(data_list, target_list)\n",
    "        \n",
    "    def sent_to_ids(self, sent_text):\n",
    "        \"\"\"\n",
    "        convert words into ids, \n",
    "        this takes in sentences, not emails\n",
    "        Then tokenize and convert to ids for each sentence\n",
    "        \"\"\"        \n",
    "        tokens = tokenizer.tokenize(sent_text)\n",
    "        \n",
    "        if self.use_unk:\n",
    "            tmp = [self.word2id[x] if x in self.word2id else self.word2id['<unk>'] for x in tokens]\n",
    "        else:\n",
    "            tmp = [self.word2id[x] for x in tokens if x in self.word2id]\n",
    "        if len(tmp) == 0:\n",
    "            tmp = [self.word2id['<empty>']]\n",
    "            self.num_empty_lines += 1\n",
    "\n",
    "        # PADDING\n",
    "        if len(tmp) > self.sent_pad_len:\n",
    "            tmp = tmp[: self.sent_pad_len]\n",
    "        text_len = max(len(tmp), 1)\n",
    "\n",
    "        tmp = tmp + [self.pad_int] * (self.sent_pad_len - len(tmp))\n",
    "        # no need to pad emoji\n",
    "        a_emoji = emoji_st.tokenize_sentences([sent_text])[0].reshape((-1)).astype(np.int64)\n",
    "\n",
    "        return tmp, text_len, a_emoji            \n",
    "\n",
    "    def email_to_ids(self, email_text):\n",
    "        \"\"\"\n",
    "        convert words into ids, \n",
    "        first split email into list of sentences, then use sent_to_ids\n",
    "        Then tokenize and convert to ids for each sentence\n",
    "        \"\"\" \n",
    "        tmp = []\n",
    "        tmp_len = []\n",
    "        a_emoji = []\n",
    "        \n",
    "        for sents in email_text:\n",
    "            keep, sents = clean_sentences(sents)\n",
    "            if keep:                \n",
    "                a, a_len, emoji_sent = self.sent_to_ids(sents)\n",
    "                tmp.append(a)\n",
    "                tmp_len.append(a_len)\n",
    "                a_emoji.append(emoji_sent)\n",
    "            \n",
    "        if len(email_text) > self.emai_pad_len:\n",
    "            tmp = tmp[: self.emai_pad_len]\n",
    "            tmp_len = tmp_len[: self.emai_pad_len]\n",
    "            a_emoji = a_emoji[: self.emai_pad_len]\n",
    "        \n",
    "        fill_times = self.emai_pad_len - len(tmp)          \n",
    "        # have to make sure the fillers look accurate for it to work\n",
    "        tmp = tmp +  [[self.pad_int] * self.sent_pad_len for i in range(fill_times)]      \n",
    "        tmp_len = tmp_len + [1] *  fill_times    \n",
    "        a_emoji = a_emoji + [[self.pad_int]  * EMOJ_SENT_PAD_LEN for i in range(fill_times)]  \n",
    " \n",
    "        return tmp, tmp_len, a_emoji\n",
    "\n",
    "    def read_data(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        data_list: contains both cleaned and raw data\n",
    "        \"\"\"\n",
    "        assert len(data_list) == len(target_list)\n",
    "\n",
    "        for X, y in zip(data_list, target_list):\n",
    "            \n",
    "            # convert clean sentence to ids\n",
    "            a, a_len, a_emoji = self.email_to_ids(X)\n",
    "\n",
    "            self.a.append(a)\n",
    "            self.a_len.append(a_len)            \n",
    "            self.emoji_a.append(a_emoji)\n",
    "\n",
    "            # append the target\n",
    "            self.e_c.append(int(y)) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.a)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return  torch.LongTensor(self.a[idx]), \\\n",
    "            torch.LongTensor(self.a_len[idx]),\\\n",
    "            torch.LongTensor(self.emoji_a[idx]),\\\n",
    "            torch.LongTensor([self.e_c[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Process test dataset, the difference is this does not read labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list,  emai_pad_len, sent_pad_len,  word2id, id2word, use_unk=False):\n",
    "\n",
    "        self.sent_pad_len = sent_pad_len\n",
    "        self.emai_pad_len = emai_pad_len\n",
    "        self.word2id = word2id\n",
    "        self.pad_int = word2id['<pad>']\n",
    "\n",
    "        self.use_unk = use_unk\n",
    "\n",
    "        # internal data\n",
    "        self.a = []\n",
    "        self.a_len = []\n",
    "        self.emoji_a = []\n",
    "\n",
    "\n",
    "        self.num_empty_lines = 0\n",
    "        # prepare dataset\n",
    "        self.ex_word2id = copy.deepcopy(word2id)\n",
    "        self.ex_id2word = copy.deepcopy(id2word)\n",
    "        self.unk_words_idx = set()\n",
    "        self.read_data(data_list)\n",
    "   \n",
    "        \n",
    "    def sent_to_ids(self, sent_text):\n",
    "        \"\"\"\n",
    "        convert words into ids, \n",
    "        this takes in sentences, not emails\n",
    "        Then tokenize and convert to ids for each sentence\n",
    "        \"\"\"        \n",
    "        tokens = tokenizer.tokenize(sent_text)\n",
    "        \n",
    "        \n",
    "        if self.use_unk:\n",
    "            tmp = [self.word2id[x] if x in self.word2id else self.word2id['<unk>'] for x in tokens]\n",
    "        else:\n",
    "            tmp = [self.word2id[x] for x in tokens if x in self.word2id]\n",
    "        if len(tmp) == 0:\n",
    "            tmp = [self.word2id['<empty>']]\n",
    "            self.num_empty_lines += 1\n",
    "\n",
    "        # PADDING\n",
    "        if len(tmp) > self.sent_pad_len:\n",
    "            tmp = tmp[: self.sent_pad_len]\n",
    "        text_len = max(len(tmp),1)\n",
    "\n",
    "        tmp = tmp + [self.pad_int] * (self.sent_pad_len - len(tmp))\n",
    "        # no need to pad emoji\n",
    "        a_emoji = emoji_st.tokenize_sentences([sent_text])[0].reshape((-1)).astype(np.int64)\n",
    "\n",
    "        return tmp, text_len, a_emoji\n",
    "        \n",
    "\n",
    "    def email_to_ids(self, email_text):\n",
    "        \"\"\"\n",
    "        convert words into ids, \n",
    "        first split email into list of sentences, then use sent_to_ids\n",
    "        Then tokenize and convert to ids for each sentence\n",
    "        \"\"\" \n",
    "        tmp = []\n",
    "        tmp_len = []\n",
    "        a_emoji = []\n",
    "                \n",
    "        for sents in email_text:\n",
    "            keep, sents = clean_sentences(sents)\n",
    "            if keep:                \n",
    "                a, a_len, emoji_sent = self.sent_to_ids(sents)\n",
    "                tmp.append(a)\n",
    "                tmp_len.append(a_len)\n",
    "                a_emoji.append(emoji_sent)\n",
    "            \n",
    "        if len(email_text) > self.emai_pad_len:\n",
    "            tmp = tmp[: self.emai_pad_len]\n",
    "            tmp_len = tmp_len[: self.emai_pad_len]\n",
    "            a_emoji = a_emoji[: self.emai_pad_len]\n",
    "            \n",
    "            \n",
    "        fill_times = self.emai_pad_len - len(tmp)    \n",
    "        \n",
    "        tmp = tmp +  [[self.pad_int] * self.sent_pad_len for i in range(fill_times)]      \n",
    "        tmp_len = tmp_len + [1] *  fill_times    \n",
    "        a_emoji = a_emoji + [[self.pad_int]  * EMOJ_SENT_PAD_LEN for i in range(fill_times)]  \n",
    " \n",
    "        return tmp, tmp_len, a_emoji\n",
    "\n",
    "\n",
    "    def read_data(self, data_list):\n",
    "        \"\"\"\n",
    "        data_list: originally contains both cleaned and raw data\n",
    "        since raw data not used, get rid of it\n",
    "        \"\"\"        \n",
    "        for X in data_list:\n",
    "\n",
    "            a, a_len, a_emoji = self.email_to_ids(X)\n",
    "\n",
    "            self.a.append(a)\n",
    "            self.a_len.append(a_len)            \n",
    "            self.emoji_a.append(a_emoji)\n",
    "            \n",
    "        print('num of empty lines,', self.num_empty_lines)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.a) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.a[idx]),\\\n",
    "               torch.LongTensor([self.a_len[idx]]), \\\n",
    "               torch.LongTensor(self.emoji_a[idx])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 8106 words\n"
     ]
    }
   ],
   "source": [
    "data_list, target_list = load_data_context(data_path='/data/SuperMod/test_data.csv')\n",
    "X = data_list # read train data list\n",
    "y = target_list\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "emoji_st = SentenceTokenizer(vocabulary, EMOJ_SENT_PAD_LEN)\n",
    "\n",
    "combined = list(zip(X, y))\n",
    "# random.shuffle(combined)\n",
    "X[:], y[:] = zip(*combined)\n",
    "\n",
    "word2id, id2word, num_of_vocab = build_vocab([data_list], VOCAB_SIZE)\n",
    "\n",
    "# build Glove embedding\n",
    "# emb = build_embedding(id2word, GLOVE_EMB_PATH, num_of_vocab)\n",
    "train_data_set = TrainDataSet(X, y, EMAI_PAD_LEN, SENT_PAD_LEN, word2id, use_unk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i, j in enumerate(train_data_set):\n",
    "#     print(j[3].squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "806"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of empty lines, 140\n"
     ]
    }
   ],
   "source": [
    "test_data_set = TestDataSet(X, EMAI_PAD_LEN, SENT_PAD_LEN, word2id, id2word, use_unk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of empty lines, 0\n"
     ]
    }
   ],
   "source": [
    "train_data_set = TrainDataSet(X, y, EMAI_PAD_LEN, SENT_PAD_LEN, word2id, use_unk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41, 13, 22, 0, 0, 0, 0, 0, 0, 0], [32, 0, 0, 0, 0, 0, 0, 0, 0, 0], [27, 37, 32, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(test_data_set.a[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to test result\n",
    "# for index, (a,b,c,d) in enumerate(train_data_set):\n",
    "#     print(a,b,c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding(id2word, fname, num_of_vocab):\n",
    "    \"\"\"\n",
    "    Build Glove Embedding, fname is the glove embedding path\n",
    "    \"\"\"\n",
    "    import io\n",
    "\n",
    "    def load_vectors(fname):\n",
    "        print(\"Loading Glove Model\")\n",
    "        f = open(fname, 'r', encoding='utf8')\n",
    "        model = {}\n",
    "        #$$$$$$$$$$$$$$$$$Need to be updated$$$$$$$$$$$$$$$$$$$$$\n",
    "        for line in tqdm(f.readlines(), total=2196017):\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            try:\n",
    "                embedding = np.array(values[1:], dtype=np.float32)\n",
    "                model[word] = embedding\n",
    "            except ValueError:\n",
    "                print(len(values), values[0])\n",
    "\n",
    "        print(\"Done.\", len(model), \" words loaded!\")\n",
    "        f.close()\n",
    "        return model\n",
    "\n",
    "    def get_emb(emb_dict, vocab_size, embedding_dim):\n",
    "\n",
    "        all_embs = np.stack(emb_dict.values())\n",
    "        emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "        emb = np.random.normal(emb_mean, emb_std, (vocab_size, embedding_dim))\n",
    "\n",
    "        num_found = 0\n",
    "        print('loading glove')\n",
    "        for idx in tqdm(range(vocab_size)):\n",
    "            word = id2word[idx]\n",
    "            if word == '<pad>' or word == '<unk>':\n",
    "                emb[idx] = np.zeros([embedding_dim])\n",
    "            elif word in emb_dict:\n",
    "                emb[idx] = emb_dict[word]\n",
    "                num_found += 1\n",
    "\n",
    "        return emb, num_found\n",
    "\n",
    "    pkl_path = fname + '.pkl'\n",
    "    if not os.path.isfile(pkl_path):\n",
    "        print('creating pkl file for the emb text file')\n",
    "        emb_dict = load_vectors(fname)\n",
    "        with open(pkl_path, 'wb') as f:\n",
    "            pkl.dump(emb_dict, f)\n",
    "    else:\n",
    "        print('loading pkl file')\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            emb_dict = pkl.load(f)\n",
    "        print('loading finished')\n",
    "\n",
    "    emb, num_found = get_emb(emb_dict, num_of_vocab, SENT_EMB_DIM)\n",
    "\n",
    "    print(num_found, 'of', num_of_vocab, 'found', 'coverage', num_found/num_of_vocab)\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pkl file\n",
      "loading finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:27: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "100%|██████████| 100/100 [00:00<00:00, 214213.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading glove\n",
      "97 of 100 found coverage 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "GLOVE_EMB_PATH = '/data/glove/glove.840B.300d.txt'\n",
    "emb = build_embedding(id2word, GLOVE_EMB_PATH, num_of_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_vocab = VOCAB_SIZE\n",
    "train_file = '/data/SuperMod/test_data.csv'\n",
    "# read data into lists\n",
    "data_list, target_list = load_data_context(data_path=train_file)\n",
    "\n",
    "# dev set\n",
    "#     dev_file = 'data/dev.txt'\n",
    "dev_file = '/data/SuperMod/test_data.csv'\n",
    "# read data into lists    \n",
    "dev_data_list, dev_target_list = load_data_context(data_path=dev_file)\n",
    "\n",
    "# test set\n",
    "#     test_file = 'data/test.txt'\n",
    "test_file = '/data/SuperMod/test_data.csv'\n",
    "test_data_list, test_target_list = load_data_context(data_path=test_file)\n",
    "\n",
    "# load final test data\n",
    "#     final_test_file = 'data/testwithoutlabels.txt'\n",
    "final_test_file = '/data/SuperMod/test_data.csv'\n",
    "final_test_data_list = load_data_context(data_path=final_test_file, is_train=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 8106 words\n",
      "filling vocab to 8109\n"
     ]
    }
   ],
   "source": [
    "word2id, id2word, num_of_vocab = build_vocab([data_list, dev_data_list, test_data_list], num_of_vocab,\n",
    "                                                 FILL_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pkl file\n",
      "loading finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:27: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "100%|██████████| 10363/10363 [00:00<00:00, 290209.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading glove\n",
      "9639 of 10363 found coverage 0.9301360609862009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "emb = build_embedding(id2word, GLOVE_EMB_PATH, num_of_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of empty lines, 0\n",
      "Size of test data 806\n"
     ]
    }
   ],
   "source": [
    "gold_dev_data_set = TestDataSet(dev_data_list, EMAI_PAD_LEN, SENT_PAD_LEN, word2id, id2word, use_unk=False)\n",
    "gold_dev_data_loader = DataLoader(gold_dev_data_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(\"Size of test data\", len(gold_dev_data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of empty lines, 0\n",
      "Size of test data 806\n",
      "num of empty lines, 0\n",
      "Size of final test data 806\n"
     ]
    }
   ],
   "source": [
    "test_data_set = TestDataSet(test_data_list, EMAI_PAD_LEN, SENT_PAD_LEN, word2id, id2word, use_unk=False)\n",
    "test_data_loader = DataLoader(test_data_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(\"Size of test data\", len(test_data_set))\n",
    "# ex_id2word, unk_words_idx = test_data_set.get_ex_id2word_unk_words()\n",
    "\n",
    "# convert to TestData class\n",
    "# then use Dataloader from torch.utils.data to create batches\n",
    "final_test_data_set = TestDataSet(final_test_data_list, EMAI_PAD_LEN, SENT_PAD_LEN, word2id, id2word, use_unk=False)\n",
    "final_test_data_loader = DataLoader(final_test_data_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(\"Size of final test data\", len(final_test_data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_tokenizer(ids, __id2word):\n",
    "    \"\"\"\n",
    "    This function is only called in elmo. Elmo encode is then called\n",
    "    _id2word: returned by build vocab\n",
    "    This simply uses id returned from vocab building function \n",
    "    \"\"\"\n",
    "    return [__id2word[int(x)] for x in ids if x != 0]\n",
    "\n",
    "def elmo_encode(data, __id2word=id2word):\n",
    "    \"\"\"\n",
    "    get the id2word from vocab, then convert to id\n",
    "    from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "    Elmo needs to be trained?\n",
    "    \"\"\"\n",
    "    data_text = [glove_tokenizer(x, __id2word) for x in data]\n",
    "    sent_pad_len = SENT_PAD_LEN\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        character_ids = batch_to_ids(data_text).cuda()\n",
    "        elmo_emb = elmo(character_ids)['elmo_representations']\n",
    "        elmo_emb = (elmo_emb[0] + elmo_emb[1]) / 2  # avg of two layers\n",
    "        sent_len = elmo_emb.shape[1]\n",
    "        row_num =  elmo_emb.shape[0]\n",
    "        elmo_dim = elmo_emb.shape[2]\n",
    "        if sent_len < sent_pad_len:\n",
    "            fill_sent_len = sent_pad_len - sent_len\n",
    "            filler = torch.zeros([row_num,fill_sent_len,elmo_dim], dtype=torch.float)\n",
    "            elmo_emb = torch.cat((elmo_emb, filler.cuda()), dim=1)\n",
    "    return elmo_emb.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_pad_len = SENT_PAD_LEN\n",
    "# data_text = [glove_tokenizer(x, id2word) for x in a[0]]\n",
    "# character_ids = batch_to_ids(data_text).cuda()\n",
    "# elmo_emb = elmo(character_ids)['elmo_representations']\n",
    "# elmo_emb = (elmo_emb[0] + elmo_emb[1]) / 2  # avg of two layers\n",
    "# sent_len = elmo_emb.shape[1]\n",
    "# row_num =  elmo_emb.shape[0]\n",
    "# elmo_dim = elmo_emb.shape[2]\n",
    "# if sent_len < sent_pad_len:\n",
    "#     fill_sent_len = sent_pad_len - sent_len\n",
    "#     filler = torch.zeros([row_num,fill_sent_len,elmo_dim], dtype=torch.float)\n",
    "#     elmo_emb = torch.cat((elmo_emb, filler.cuda()), dim=1)\n",
    "# elmo_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_list # read train data list\n",
    "y = target_list\n",
    "y = np.array(y)\n",
    "\n",
    "combined = list(zip(X, y))\n",
    "random.shuffle(combined)\n",
    "X[:], y[:] = zip(*combined)\n",
    "\n",
    "# train dev split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=NUM_OF_FOLD, random_state=0)\n",
    "all_fold_results = []\n",
    "real_test_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.296875"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_set)/BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we look at one fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fold = 1\n",
    "train_index = []\n",
    "dev_index = []\n",
    "\n",
    "for idx, (_train_index, _dev_index) in enumerate(skf.split(X, y)):\n",
    "    num_fold = idx\n",
    "    train_index = _train_index[:20]\n",
    "    dev_index = _dev_index[:20]\n",
    "    \n",
    "# train, dev indices only contain indexes\n",
    "# by doing this, we grab the last fold to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of empty lines, 0\n",
      "num of empty lines, 0\n"
     ]
    }
   ],
   "source": [
    " \n",
    "X_train, X_dev = [X[i] for i in train_index], [X[i] for i in dev_index]\n",
    "y_train, y_dev = y[train_index], y[dev_index]\n",
    "\n",
    "# construct data loader\n",
    "# for one fold, test data comes from k fold split.\n",
    "train_data_set = TrainDataSet(X_train, y_train, EMAI_PAD_LEN, SENT_PAD_LEN, word2id, use_unk=True)\n",
    "\n",
    "dev_data_set = TrainDataSet(X_dev, y_dev, EMAI_PAD_LEN, SENT_PAD_LEN, word2id, use_unk=True)\n",
    "dev_data_loader = DataLoader(dev_data_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pred_list_test_best = None\n",
    "final_pred_best = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "print out for testing\n",
    "for i, j in enumerate(train_data_set):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_set[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "train_data_loader = DataLoader(train_data_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (a,b,c,d) in enumerate(train_data_loader) :\n",
    "    last_a = a\n",
    "    last_b = b\n",
    "    last_c = c\n",
    "    last_d = d\n",
    "# again we grab the last batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   3, 3255, 3256,   13,   57,  191,  121,   32,    0,    0],\n",
       "         [   3,    3,    7,   14,  967,   34,   37,  806,  123,   32],\n",
       "         [ 133,  228,   85,   84, 1943,   37,  277,    5,   14,    0]],\n",
       "\n",
       "        [[  15, 3160, 1360, 1483,  130,   15,  528,   77,  665,    6],\n",
       "         [  73, 5673,  109,   62,  140, 4609,  490,  245,    5,    0],\n",
       "         [ 240,    9, 5674,    6,   79,  529, 5675,   89,   79, 3300]],\n",
       "\n",
       "        [[  12,  142,   42, 1496,  141,   18,   39,    9, 6965, 1505],\n",
       "         [  62,  134,   30,   73,   41,   15,  558,   19, 4142,    5],\n",
       "         [  62,  320,  134,   62,   25,  404,   30,   28,    5,    0]],\n",
       "\n",
       "        [[   4,    4,    4,    4,    4,    4,    4,    4,    4,    4],\n",
       "         [1979, 2882,    8,   87,  804,   24,  281,   84,  717, 2883],\n",
       "         [   3,    3,   18,    5,    0,    0,    0,    0,    0,    0]],\n",
       "\n",
       "        [[ 241,    4,   13,   40,   10,  150,   48,  121,  116,   32],\n",
       "         [   4,    4,    4,    4,    4,   50,   46,    4,    4,    4],\n",
       "         [8894,  135,  285,  116,   34, 3613,    5,    0,    0,    0]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 133,  228,   85,   84, 1943,   37,  277,    5,   14,    0],\n",
       "        [ 240,    9, 5674,    6,   79,  529, 5675,   89,   79, 3300],\n",
       "        [  62,  320,  134,   62,   25,  404,   30,   28,    5,    0],\n",
       "        [   3,    3,   18,    5,    0,    0,    0,    0,    0,    0],\n",
       "        [8894,  135,  285,  116,   34, 3613,    5,    0,    0,    0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_a[:,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8, 10,  9],\n",
       "        [10,  9, 10],\n",
       "        [10, 10,  9],\n",
       "        [10, 10,  4],\n",
       "        [10, 10,  7]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   3, 3255, 3256,   13,   57,  191,  121,   32,    0,    0],\n",
      "        [   3,    3,    7,   14,  967,   34,   37,  806,  123,   32],\n",
      "        [ 133,  228,   85,   84, 1943,   37,  277,    5,   14,    0]])\n",
      "next\n",
      "tensor([[  15, 3160, 1360, 1483,  130,   15,  528,   77,  665,    6],\n",
      "        [  73, 5673,  109,   62,  140, 4609,  490,  245,    5,    0],\n",
      "        [ 240,    9, 5674,    6,   79,  529, 5675,   89,   79, 3300]])\n",
      "next\n",
      "tensor([[  12,  142,   42, 1496,  141,   18,   39,    9, 6965, 1505],\n",
      "        [  62,  134,   30,   73,   41,   15,  558,   19, 4142,    5],\n",
      "        [  62,  320,  134,   62,   25,  404,   30,   28,    5,    0]])\n",
      "next\n",
      "tensor([[   4,    4,    4,    4,    4,    4,    4,    4,    4,    4],\n",
      "        [1979, 2882,    8,   87,  804,   24,  281,   84,  717, 2883],\n",
      "        [   3,    3,   18,    5,    0,    0,    0,    0,    0,    0]])\n",
      "next\n",
      "tensor([[ 241,    4,   13,   40,   10,  150,   48,  121,  116,   32],\n",
      "        [   4,    4,    4,    4,    4,   50,   46,    4,    4,    4],\n",
      "        [8894,  135,  285,  116,   34, 3613,    5,    0,    0,    0]])\n",
      "next\n"
     ]
    }
   ],
   "source": [
    "for i in last_a:\n",
    "    print(i)\n",
    "    print(\"next\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (a, a_len, emoji_a, e_c) \\\n",
    "#                     in tqdm(enumerate(train_data_loader), total=len(train_data_set)/BATCH_SIZE):\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HierarchicalAttPredictor(SENT_EMB_DIM, SENT_HIDDEN_SIZE, 100, num_of_vocab,SENT_PAD_LEN , id2word, USE_ELMO=True, ADD_LINEAR=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HierarchicalAttPredictor' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 539\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HierarchicalAttPredictor' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HierarchicalAttPredictor(SENT_EMB_DIM, SENT_HIDDEN_SIZE, CTX_LSTM_DIM, num_of_vocab, SENT_PAD_LEN , id2word, USE_ELMO=True, ADD_LINEAR=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for embed.weight\n",
      "Loading weights for lstm_0.weight_ih_l0\n",
      "Loading weights for lstm_0.weight_hh_l0\n",
      "Loading weights for lstm_0.bias_ih_l0\n",
      "Loading weights for lstm_0.bias_hh_l0\n",
      "Loading weights for lstm_0.weight_ih_l0_reverse\n",
      "Loading weights for lstm_0.weight_hh_l0_reverse\n",
      "Loading weights for lstm_0.bias_ih_l0_reverse\n",
      "Loading weights for lstm_0.bias_hh_l0_reverse\n",
      "Loading weights for lstm_1.weight_ih_l0\n",
      "Loading weights for lstm_1.weight_hh_l0\n",
      "Loading weights for lstm_1.bias_ih_l0\n",
      "Loading weights for lstm_1.bias_hh_l0\n",
      "Loading weights for lstm_1.weight_ih_l0_reverse\n",
      "Loading weights for lstm_1.weight_hh_l0_reverse\n",
      "Loading weights for lstm_1.bias_ih_l0_reverse\n",
      "Loading weights for lstm_1.bias_hh_l0_reverse\n",
      "Loading weights for attention_layer.attention_vector\n",
      "Ignoring weights for output_layer.0.weight\n",
      "Ignoring weights for output_layer.0.bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HierarchicalAttPredictor(\n",
       "  (deepmoji_model): TorchMoji(\n",
       "    (embed): Embedding(50000, 256)\n",
       "    (embed_dropout): Dropout2d(p=0.2)\n",
       "    (lstm_0): LSTMHardSigmoid(256, 512, batch_first=True, bidirectional=True)\n",
       "    (lstm_1): LSTMHardSigmoid(1024, 512, batch_first=True, bidirectional=True)\n",
       "    (attention_layer): AttentionOneParaPerChan(2304)\n",
       "    (final_dropout): Dropout2d(p=0.2)\n",
       "  )\n",
       "  (deepmoji2linear): Linear(in_features=2304, out_features=300, bias=True)\n",
       "  (a_lstm): LSTM(1324, 1500, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (a_self_attention): AttentionOneParaPerChan(3000)\n",
       "  (context_lstm): LSTM(5304, 800, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (ctx_self_attention): AttentionOneParaPerChan(1600)\n",
       "  (embeddings): Embedding(10363, 300, padding_idx=0)\n",
       "  (context_to_emo): Linear(in_features=800, out_features=2, bias=True)\n",
       "  (drop_out): Dropout(p=0.2)\n",
       "  (out2label): Linear(in_features=1600, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_embedding(emb)\n",
    "model.deepmoji_model.load_specific_weights(PRETRAINED_PATH, exclude_names=['output_layer'])\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True) #\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "# if opt.w == 1:\n",
    "#     weight_list = [0.3, 0.3, 0.3, 1.7]\n",
    "#     weight_list_binary = [2 - weight_list[-1], weight_list[-1]]\n",
    "# elif opt.w == 2:\n",
    "#     weight_list = [0.3198680179, 0.246494733, 0.2484349259, 1.74527696]\n",
    "#     weight_list_binary = [2 - weight_list[-1], weight_list[-1]]\n",
    "# else:\n",
    "#     raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification reweight:  [1, 1]\n"
     ]
    }
   ],
   "source": [
    "if w == 1:\n",
    "    weight_list = [1, 1]\n",
    "elif w == 2:\n",
    "    weight_list = [1, 1]\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "weight_list = [x**FLAT for x in weight_list]\n",
    "weight_label = torch.Tensor(weight_list).cuda()\n",
    "# weight is multiplied directly to the loss label\n",
    "\n",
    "print('classification reweight: ', weight_list)\n",
    "\n",
    "# loss_criterion_binary = nn.CrossEntropyLoss(weight=weight_list_binary)  #\n",
    "if loss == 'focal':\n",
    "    loss_criterion = FocalLoss(gamma= focal )\n",
    "\n",
    "elif loss == 'ce':\n",
    "    loss_criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "es = EarlyStopping(patience=EARLY_STOP_PATIENCE)\n",
    "# best_model = None\n",
    "final_pred_list_test = None\n",
    "pred_list_test = None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training epoch: 0...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4.0 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate [0.0005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4.0 [00:00<00:01,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "tensor(0.8371, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4.0 [00:00<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "tensor(0.5544, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4.0 [00:01<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "tensor(0.2898, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4.0 [00:01<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "tensor(0.4843, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n",
      "Begin training epoch: 1...\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4.0 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate [2e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4.0 [00:00<00:01,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "tensor(0.7421, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4.0 [00:00<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "tensor(0.3112, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4.0 [00:01<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "tensor(0.2881, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4.0 [00:01<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "tensor(0.5021, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for num_epoch in range(2):\n",
    "    # to ensure shuffle at ever epoch\n",
    "    train_data_loader = DataLoader(train_data_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print('Begin training epoch:', num_epoch, end='...\\t')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # stepping scheduler\n",
    "    scheduler.step(num_epoch)\n",
    "    print('Current learning rate', scheduler.get_lr())\n",
    "\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for i, (a, a_len, emoji_a, e_c) \\\n",
    "            in tqdm(enumerate(train_data_loader), total=len(train_data_set)/BATCH_SIZE):\n",
    "        \n",
    "        e_c = e_c.type(torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(a.cuda(), a_len, emoji_a.cuda())\n",
    "        print(e_c.type())\n",
    "        loss_label = loss_criterion(pred, e_c.view(-1).cuda()).cuda()\n",
    "        print(loss_label)\n",
    "#         e_c = e_c.type(torch.LongTensor)        \n",
    "#         loss_label = torch.matmul(torch.gather(weight_label, 0, e_c.view(-1).cuda()), loss_label) / \\\n",
    "#                      e_c.view(-1).shape[0]\n",
    "\n",
    "    \n",
    "        # training trilogy\n",
    "        loss_label.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss_label.data.cpu().numpy() * a.shape[0]\n",
    "#         del pred, loss, elmo_a, elmo_b, elmo_c, e_c_emo, loss_binary, loss_label, loss_emo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7581],\n",
       "        [0.7615],\n",
       "        [0.7605],\n",
       "        [0.7549],\n",
       "        [0.7549]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pred\n",
    "ground = np.array([1 if x == 1 else 0 for x in e_c ])\n",
    "THRESHHOLD = 0.5\n",
    "discretePredictions = np.array([1  for x in predictions if x >= THRESHHOLD])\n",
    "truePositives = np.sum([1 for (x,y) in zip(discretePredictions, ground) if x * y == 1])\n",
    "falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "falseNegatives = np.sum(np.clip(ground - discretePredictions, 0, 1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falsePositives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(ground, predictions):\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref -\n",
    "        https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification\n",
    "    \"\"\"\n",
    "    ground = np.array([1 if x == 1 else 0 for x in ground ])\n",
    "    discretePredictions = np.array([1  for x in predictions if x >= THRESHHOLD])\n",
    "    truePositives = np.sum([1 for (x,y) in zip(discretePredictions, ground) if x * y == 1])\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground - discretePredictions, 0, 1), axis=0)\n",
    "\n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)\n",
    "\n",
    "    #  Macro level calculation\n",
    "\n",
    "    precision = truePositives / (truePositives + falsePositives)\n",
    "    recall = truePositives/ (truePositives + falseNegatives)\n",
    "    accuracy = np.mean(discretePredictions == ground)\n",
    "    f1 = (2 * recall * precision) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(\"Accuracy : %.4f, Precision : %.3f, Recall : %.3f, F1 : %.3f\" % (accuracy,  precision, recall, f1))\n",
    "\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  4\n",
      "False Positives per class :  1\n",
      "False Negatives per class :  0\n",
      "Accuracy : 0.8000, Precision : 0.800, Recall : 1.000, F1 : 0.889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8, 0.8, 1.0, 0.888888888888889)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(e_c, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now test the actual model\n",
    "## Start\n",
    "HierarchicalPredictor(SENT_EMB_DIM, SENT_HIDDEN_SIZE, num_of_vocab, USE_ELMO=True, ADD_LINEAR=False)\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, USE_ELMO, ADD_LINEAR):\n",
    "\n",
    "last_a = a  \n",
    "last_b = b  \n",
    "last_c = c   \n",
    "last_d = d  \n",
    "\n",
    "\n",
    "for i, (a, a_len, emoji_a, e_c) \\\n",
    "                in tqdm(enumerate(train_data_loader), total=len(train_data_set)/BATCH_SIZE):\n",
    "            optimizer.zero_grad()\n",
    "            elmo_a = elmo_encode(a)\n",
    "\n",
    "            pred = model(a.cuda(), a_len, emoji_a.cuda(), elmo_a)\n",
    "\n",
    "            # compare predicted with the actual label\n",
    "            loss_label = loss_criterion(pred, e_c.view(-1).cuda()).cuda()\n",
    "            # The torch.gather function is a multi-index selection method\n",
    "            loss_label = torch.matmul(torch.gather(weight_label, 0, e_c.view(-1).cuda()), loss_label) / \\\n",
    "                         e_c.view(-1).shape[0]\n",
    "\n",
    "            # weighting the different losses. For email, just use the label\n",
    "            loss = loss_label \n",
    "\n",
    "            # loss = torch.matmul(torch.gather(weight, 0, trg.view(-1).cuda()), loss) / trg.view(-1).shape[0]\n",
    "\n",
    "            # training trilogy\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.data.cpu().numpy() * a.shape[0]\n",
    "            del pred, loss, elmo_a, loss_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########\n",
    "# ####### After testing, do not run the following\n",
    "# import numpy as np\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch\n",
    "# from torch.autograd import Variable\n",
    "# from module.self_attention import BertAttention, AttentionOneParaPerChan\n",
    "# from module.torch_moji import TorchMoji\n",
    "\n",
    "# import pickle as pkl\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# embedding_dim = SENT_EMB_DIM\n",
    "# hidden_dim = SENT_HIDDEN_SIZE\n",
    "# SENT_LSTM_DIM = SENT_HIDDEN_SIZE\n",
    "# bidirectional = True\n",
    "# add_linear = True\n",
    "\n",
    "# sent_lstm_directions = 2 if bidirectional else 1\n",
    "# # centence level lstm attention\n",
    "# cent_lstm_att_fn = AttentionOneParaPerChan\n",
    "# # context lstm attention\n",
    "# ctx_lstm__att_fn = AttentionOneParaPerChan\n",
    "\n",
    "# NUM_EMO = 2\n",
    "\n",
    "\n",
    "# deepmoji_model = TorchMoji(nb_classes=None,\n",
    "#                                 nb_tokens=50000,\n",
    "#                                 embed_dropout_rate=0.2,\n",
    "#                                 final_dropout_rate=0.2)\n",
    "# deepmoji_dim = 2304\n",
    "# deepmoji_out = 300\n",
    "# deepmoji2linear = nn.Linear(deepmoji_dim, deepmoji_out)\n",
    "\n",
    "# elmo_dim = 1024\n",
    "\n",
    "# num_layers = 2\n",
    "# use_elmo = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ########\n",
    "# # ####### After testing, do not run the following\n",
    "# a_lstm = nn.LSTM(embedding_dim + elmo_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=0.2) \n",
    "# a_self_attention = cent_lstm_att_fn(sent_lstm_directions*hidden_dim)\n",
    "# ctx_bidirectional = True\n",
    "# ctx_lstm_dim = 800\n",
    "# ctx_lstm_directions = 2 if ctx_bidirectional else 1\n",
    "\n",
    "# if not add_linear:\n",
    "#     deepmoji_out = deepmoji_dim\n",
    "\n",
    "# context_lstm = nn.LSTM(SENT_LSTM_DIM * sent_lstm_directions + deepmoji_out, ctx_lstm_dim,\n",
    "#                                     num_layers=2, batch_first=True, dropout=0.2, bidirectional=ctx_bidirectional)\n",
    "# ctx_self_attention = ctx_lstm__att_fn(ctx_lstm_directions * ctx_lstm_dim)\n",
    "\n",
    "# vocab_size = num_of_vocab\n",
    "# embedding_dim = embedding_dim\n",
    "# embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # create a linear model that goes from context lstm dim to prediction\n",
    "# context_to_emo = nn.Linear(ctx_lstm_dim, 2)\n",
    "# drop_out = nn.Dropout(0.2)\n",
    "# out2label = nn.Linear(ctx_lstm_dim * ctx_lstm_directions, 1)\n",
    "\n",
    "# embeddings.cuda()\n",
    "# context_lstm.cuda()\n",
    "# a_lstm.cuda()\n",
    "# # deepmoji_model.cuda()\n",
    "# # a_self_attention.cuda()\n",
    "# deepmoji2linear.cuda()\n",
    "# out2label.cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # ########\n",
    "# # # ####### After testing, do not run the following\n",
    "\n",
    "# def init_hidden( x):\n",
    "#     batch_size = x.size(0)\n",
    "#     if bidirectional:\n",
    "#         h0 = Variable(torch.zeros(2*num_layers, batch_size, SENT_LSTM_DIM), requires_grad=False).cuda()\n",
    "#         c0 = Variable(torch.zeros(2*num_layers, batch_size, SENT_LSTM_DIM), requires_grad=False).cuda()\n",
    "#     else:\n",
    "#         h0 = Variable(torch.zeros(1*num_layers, batch_size, SENT_LSTM_DIM), requires_grad=False).cuda()\n",
    "#         c0 = Variable(torch.zeros(1*num_layers, batch_size, SENT_LSTM_DIM), requires_grad=False).cuda()\n",
    "#     return (h0, c0)\n",
    "\n",
    "# def sort_batch(batch, lengths):\n",
    "#     seq_lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "#     seq_tensor = batch[perm_idx]\n",
    "#     rever_sort = np.zeros(len(seq_lengths))\n",
    "#     for i, l in enumerate(perm_idx):\n",
    "#         rever_sort[l] = i\n",
    "#     return seq_tensor, seq_lengths, rever_sort.astype(int), perm_idx\n",
    "\n",
    "# def sent_lstm_forward( x, x_len, lstm, hidden=None, attention_layer=None):\n",
    "#     x, x_len_sorted, reverse_idx, perm_idx = sort_batch(x, x_len.view(-1))\n",
    "#     max_len = int(x_len_sorted[0])\n",
    "#     \"\"\"\n",
    "#     this only contains the sentence level LSTM\n",
    "#     a_out, a_hidden = self.lstm_forward(a, a_len, elmo_a, self.a_lstm,\n",
    "#                                 attention_layer=self.a_self_attention)\n",
    "#     \"\"\"\n",
    "\n",
    "#     emb_x = embeddings(x)\n",
    "#     emb_x = drop_out(emb_x)\n",
    "# #     emb_x = emb_x[:, :max_len, :]\n",
    "\n",
    "#     if use_elmo:\n",
    "#         #concatnate elmo and Glove\n",
    "#         elmo_x = elmo_encode(x)        \n",
    "#         elmo_x = elmo_x[perm_idx]\n",
    "#         elmo_x = drop_out(elmo_x)\n",
    "#         emb_x = torch.cat((emb_x.cuda(), elmo_x), dim=2)\n",
    "\n",
    "#     packed_input = nn.utils.rnn.pack_padded_sequence(emb_x, x_len_sorted.cpu().numpy(), batch_first=True)\n",
    "#     if hidden is None:\n",
    "#         hidden = init_hidden(x)\n",
    "\n",
    "#     packed_output, hidden = lstm(packed_input, hidden)\n",
    "#     output, unpacked_len = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "#     # attention_layer = None  # testing\n",
    "#     if attention_layer is None:\n",
    "#         seq_len = torch.LongTensor(unpacked_len).view(-1, 1, 1).expand(output.size(0), 1, output.size(2))\n",
    "#         seq_len = Variable(seq_len - 1).cuda()\n",
    "#         output = torch.gather(output, 1, seq_len).squeeze(1)\n",
    "#     else:\n",
    "#         if isinstance(attention_layer, AttentionOneParaPerChan):\n",
    "#             output, alpha = attention_layer(output, unpacked_len)\n",
    "#         else:\n",
    "#             unpacked_len = [int(x.data) for x in unpacked_len]\n",
    "#             # print(unpacked_len)\n",
    "#             max_len = max(unpacked_len)\n",
    "#             mask = [[1] * l + [0] * (max_len - l) for l in unpacked_len]\n",
    "#             mask = torch.FloatTensor(np.asarray(mask)).cuda()\n",
    "#             attention_mask = torch.ones_like(mask)\n",
    "#             extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "#             # extended_attention_mask = extended_attention_mask.to(\n",
    "#             #       type=next(self.parameters()).dtype)  # fp16 compatibility\n",
    "#             extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "#             output, alpha = attention_layer(output, extended_attention_mask)\n",
    "#             # out, att = self.attention_layer(lstm_out[:, -1:].squeeze(1), lstm_out)\n",
    "#             output = output[:, 0, :]\n",
    "\n",
    "#     return output[reverse_idx], (hidden[0][:, reverse_idx, :], hidden[1][:, reverse_idx, :])\n",
    "\n",
    "# def load_embedding( emb):\n",
    "#     embeddings.weight = nn.Parameter(torch.FloatTensor(emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ########\n",
    "# # ####### After testing, do not run the following\n",
    "\n",
    "# a.shape\n",
    "# a = last_a\n",
    "# a_len = last_b \n",
    "# emoji_a = last_c \n",
    "# e_c = last_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have to move elmo_encode into the model, because takes in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# elmo_a = torch.stack([elmo_encode(email) for email in a])\n",
    "# data_text = [glove_tokenizer(x, id2word) for x in a[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # ########\n",
    "# # # # ####### After testing, do not run the following\n",
    "\n",
    "# context_in = []\n",
    "# list_of_a = a\n",
    "# list_of_a_len = a_len\n",
    "# list_of_a_emoji = emoji_a\n",
    "\n",
    "# for i in range(EMAI_PAD_LEN):\n",
    "#     \"\"\"\n",
    "#     list_of_a: list of list of words\n",
    "#     list_of_a_len: list of list of lengths \n",
    "#     a: correspond to an email\n",
    "#     a_len: correspond to sentence lengths in an email\n",
    "#     \"\"\"\n",
    "#     a = list_of_a[:,i,:]\n",
    "#     a_len = list_of_a_len[:,i]\n",
    "#     a_emoji = list_of_a_emoji[:,i,:]\n",
    "\n",
    "    \n",
    "#     print(a.shape)\n",
    "#     print(\"-------------------a ----------\", a.device)\n",
    "#     print(a_len.shape)\n",
    "#     print(\"-------------------a_len ----------\",a_len.device)    \n",
    "#     print(a_emoji.shape)\n",
    "#     print(\"-------------------a_emoji ----------\", a_emoji.device)    \n",
    "\n",
    "#     # elmo_a: whether to use elmo in this model\n",
    "#     # this lstm does not have emoji\n",
    "\n",
    "#     a_out, a_hidden = sent_lstm_forward(a.cuda(), a_len, a_lstm,\n",
    "#                                         attention_layer=a_self_attention)\n",
    "    \n",
    "#     print(a_out)\n",
    "#     print(\"-------------------a_out ----------\", a_out.shape)\n",
    "    \n",
    "#     # a_out = a_out[:, 0, :]\n",
    "#     if add_linear:\n",
    "#         a_emoji = deepmoji_model(a_emoji.cuda())\n",
    "#         a_emoji = deepmoji2linear(a_emoji)\n",
    "#         a_emoji = F.relu(a_emoji)\n",
    "#         a_emoji = drop_out(a_emoji)\n",
    "#     else:\n",
    "#         a_emoji = deepmoji_model(a_emoji)\n",
    "#         a_emoji = F.relu(a_emoji)\n",
    "\n",
    "#     # a_out = torch.cat((F.relu(a_out), a_emoji), dim=1)\n",
    "#     a_out = torch.cat((a_out, a_emoji), dim=1)\n",
    "\n",
    "#     context_in.append(a_out.unsqueeze(1))\n",
    "\n",
    "# # Context LSTM\n",
    "# context_in = torch.cat(context_in, dim=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 1500])\n",
      "torch.Size([4, 5, 1500])\n"
     ]
    }
   ],
   "source": [
    "# # # # # ########\n",
    "# # # # # ####### After testing, do not run the following\n",
    "\n",
    "# a_out.shape\n",
    "# for i in a_hidden:\n",
    "#     print(i.shape)\n",
    "# context_in.shape    \n",
    "# ctx_out, _ = context_lstm(context_in)\n",
    "# ctx_out.shape\n",
    "\n",
    "# NUM_OF_ROWS = list_of_a.shape[0]\n",
    "# ctx_self_attention.cuda()\n",
    "# if isinstance(ctx_self_attention, AttentionOneParaPerChan):\n",
    "#     ctx_out, _ = ctx_self_attention(ctx_out, torch.LongTensor([EMAI_PAD_LEN for _ in range(NUM_OF_ROWS)]))\n",
    "# else:\n",
    "#     ctx_out, _ = ctx_self_attention(ctx_out)\n",
    "#     ctx_out = ctx_out[:, 0, :]  \n",
    "    \n",
    "# ctx_out.shape    \n",
    "# torch.sigmoid(out2label(ctx_out))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now test the actual model\n",
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     for i, (a, a_len, emoji_a, e_c) \\\n",
    "                        in tqdm(enumerate(train_data_loader), total=len(train_data_set)/BATCH_SIZE):\n",
    "                    optimizer.zero_grad()\n",
    "                    elmo_a = elmo_encode(a)\n",
    "\n",
    "                    pred = model(a.cuda(), a_len, emoji_a.cuda(), elmo_a)\n",
    "                    \n",
    "                    # compare predicted with the actual label\n",
    "                    loss_label = loss_criterion(pred, e_c.view(-1).cuda()).cuda()\n",
    "                    # The torch.gather function is a multi-index selection method\n",
    "                    loss_label = torch.matmul(torch.gather(weight_label, 0, e_c.view(-1).cuda()), loss_label) / \\\n",
    "                                 e_c.view(-1).shape[0]\n",
    "\n",
    "                    # weighting the different losses. For email, just use the label\n",
    "                    loss = loss_label \n",
    "\n",
    "                    # loss = torch.matmul(torch.gather(weight, 0, trg.view(-1).cuda()), loss) / trg.view(-1).shape[0]\n",
    "\n",
    "                    # training trilogy\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.data.cpu().numpy() * a.shape[0]\n",
    "                    del pred, loss, elmo_a, loss_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def forward( list_of_a, list_of_a_len, list_of_a_emoji,  elmo_a=None):        \n",
    "    \"\"\"\n",
    "    This forward the model on the email level, not sentence level\n",
    "    (a, a_len, emoji_a, e_c) in enumerate(dev_data_loader)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # it represents the number of sentences we have padded/cut off for each email\n",
    "    # we have to append results for each sentence together for the email-wide attention\n",
    "    EMAI_PAD_LEN = list_of_a.shape[1]\n",
    "    NUM_OF_ROWS = list_of_a.shape[0]\n",
    "\n",
    "    # going to concat all the output into one to be put into contex lstm\n",
    "    context_in = []\n",
    "\n",
    "    for i in range(EMAI_PAD_LEN):\n",
    "        \"\"\"\n",
    "        list_of_a: list of list of words\n",
    "        list_of_a_len: list of list of lengths \n",
    "        a: correspond to an email\n",
    "        a_len: correspond to sentence lengths in an email\n",
    "        \"\"\"\n",
    "        a = list_of_a[:,i,:]\n",
    "        a_len = list_of_a_len[:,i]\n",
    "        a_emoji = list_of_a_emoji[:,i,:]\n",
    "\n",
    "        # USE_elmo: whether to use elmo in this model\n",
    "        # this lstm does not have emoji\n",
    "        a_out, a_hidden = self.sent_lstm_forward(a, a_len, elmo_a, self.a_lstm,\n",
    "                                            attention_layer=self.a_self_attention)\n",
    "\n",
    "        # a_out = a_out[:, 0, :]\n",
    "        if self.add_linear:\n",
    "            a_emoji = self.deepmoji_model(a_emoji)\n",
    "            a_emoji = self.deepmoji2linear(a_emoji)\n",
    "            a_emoji = F.relu(a_emoji)\n",
    "            a_emoji = self.drop_out(a_emoji)\n",
    "        else:\n",
    "            a_emoji = self.deepmoji_model(a_emoji)\n",
    "            a_emoji = F.relu(a_emoji)\n",
    "\n",
    "        # a_out = torch.cat((F.relu(a_out), a_emoji), dim=1)\n",
    "        a_out = torch.cat((a_out, a_emoji), dim=1)\n",
    "\n",
    "        context_in.append(a_out.unsqueeze(1))\n",
    "\n",
    "    # Context LSTM\n",
    "    context_in = torch.cat(context_in, dim=1)\n",
    "\n",
    "    ctx_out, _ = self.context_lstm(context_in)\n",
    "    ## use contex LSTM to get the h output to be fed into Attention layer\n",
    "    if isinstance(self.ctx_self_attention, AttentionOneParaPerChan):\n",
    "        ctx_out, _ = self.ctx_self_attention(ctx_out, torch.LongTensor([ EMAI_PAD_LEN for _ in range(NUM_OF_ROWS)]))\n",
    "    else:\n",
    "        ctx_out, _ = self.ctx_self_attention(ctx_out)\n",
    "        ctx_out = ctx_out[:, 0, :]\n",
    "\n",
    "    # ctx_out = self.ctx_layer_norm(ctx_out)\n",
    "    # ctx_out = self.ctx_pooler(ctx_out)\n",
    "\n",
    "    # multi-task learning\n",
    "    return self.out2label(ctx_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_fold = 1\n",
    "# train_index = \n",
    "# dev_index\n",
    "\n",
    "# for idx, (_train_index, _dev_index) in enumerate(skf.split(X, y)):\n",
    "#     print('Train size:', len(_train_index), 'Dev size:', len(_dev_index))\n",
    "#     one_fold(idx, _train_index, _dev_index)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (a, a_len, emoji_a, e_c)  in tqdm(enumerate(train_data_loader), total=len(train_data_set)/BATCH_SIZE):\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python trainer_ha.py -glovepath '/data/glove/glove.840B.300d.txt' -folds 2 -epoch 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inference piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "Tokenizing using dictionary from /data/torchMoji/model/vocabulary.json\n",
      "loading pkl file\n",
      "loading finished\n",
      "loading glove\n",
      "100%|█████████████████████████████████| 20003/20003 [00:00<00:00, 375140.46it/s]\n",
      "15613 of 20003 found coverage 0.7805329200619907\n",
      "Traceback (most recent call last):\n",
      "  File \"inference_ha.py\", line 137, in <module>\n",
      "    main()\n",
      "  File \"inference_ha.py\", line 120, in main\n",
      "    model.cuda()\n",
      "AttributeError: 'collections.OrderedDict' object has no attribute 'cuda'\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python inference_ha.py -test_path \"/data/SuperMod/training_data_supermod.csv\"  -out_path '/data/SuperMod/hapy_state_old.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n",
      "Tokenizing using dictionary from /data/torchMoji/model/vocabulary.json\n",
      "loading pkl file\n",
      "loading finished\n",
      "loading glove\n",
      "100%|█████████████████████████████████| 24060/24060 [00:00<00:00, 365205.17it/s]\n",
      "18561 of 24060 found coverage 0.7714463840399003\n",
      "1it [02:29, 149.90s/it]                                                         \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python inference_ha.py -test_path \"/data/SuperMod/final_test.csv\"  -out_path '/data/SuperMod/hapy_state_delete.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08584689, 0.10278593, 0.08584059, 0.08583173, 0.08973207,\n",
       "       0.08585303, 0.12729162], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data/SuperMod/infer_result.pkl', 'rb') as w:\n",
    "    old_result = pkl.load(w)\n",
    "\n",
    "old_result    [:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.928647347525712,\n",
       "  0.6037838573567442,\n",
       "  0.728407224958949,\n",
       "  0.6602664285182704],\n",
       " 1: [0.9194879489824627,\n",
       "  0.5556609365737997,\n",
       "  0.7696223316912972,\n",
       "  0.6453700516351119],\n",
       " 2: [0.916049266935509,\n",
       "  0.5409780006839165,\n",
       "  0.7793103448275862,\n",
       "  0.6386328466662182]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data/SuperMod/result_old.pkl', 'rb') as w:\n",
    "    old_result = pkl.load(w)\n",
    "\n",
    "old_result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.895, nan, 0.0, 0], 1: [0.895, nan, 0.0, 0]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data/SuperMod/result_wiki.pkl', 'rb') as w:\n",
    "    old_result = pkl.load(w)\n",
    "\n",
    "old_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Jane:\\n \\nI went through your presentation.  A...</td>\n",
       "      <td>1</td>\n",
       "      <td>mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>All,\\n\\nThis was my idea. I have been suggesti...</td>\n",
       "      <td>1</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Mark:\\n\\nWe had four months to prepare for thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Jane:\\n \\nI went through your presentation.  A...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>All,\\n\\nI understand we are deciding on an ide...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Mark:\\n\\nWe had four months to prepare for thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>You are a mistake – loser!</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       comment_text  toxicity  \\\n",
       "0           0  Jane:\\n \\nI went through your presentation.  A...         1   \n",
       "1           1  All,\\n\\nThis was my idea. I have been suggesti...         1   \n",
       "2           2  Mark:\\n\\nWe had four months to prepare for thi...         1   \n",
       "3           3  Jane:\\n \\nI went through your presentation.  A...         0   \n",
       "4           4  All,\\n\\nI understand we are deciding on an ide...         0   \n",
       "5           5  Mark:\\n\\nWe had four months to prepare for thi...         0   \n",
       "6           6                         You are a mistake – loser!         1   \n",
       "\n",
       "     degree  \n",
       "0      mild  \n",
       "1  moderate  \n",
       "2  moderate  \n",
       "3      none  \n",
       "4      none  \n",
       "5      none  \n",
       "6       NaN  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf = pd.read_csv(\"/data/SuperMod/final_test.csv\")\n",
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-cab71abb309d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0melmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElmoEmbedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"an\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"apple\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"breakfast\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tired\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/allennlp/allennlp/commands/elmo.py\u001b[0m in \u001b[0;36membed_sentence\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \"\"\"\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0membed_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/allennlp/allennlp/commands/elmo.py\u001b[0m in \u001b[0;36membed_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0melmo_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_to_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/allennlp/allennlp/commands/elmo.py\u001b[0m in \u001b[0;36mbatch_to_embeddings\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0msecond\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mcharacter_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_device\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mcharacter_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharacter_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/allennlp/allennlp/modules/elmo.py\u001b[0m in \u001b[0;36mbatch_to_ids\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'elmo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'character_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/allennlp/allennlp/data/dataset.py\u001b[0m in \u001b[0;36mindex_instances\u001b[0;34m(self, vocab)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_fields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/allennlp/allennlp/data/instance.py\u001b[0m in \u001b[0;36mindex_fields\u001b[0;34m(self, vocab)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_padding_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/allennlp/allennlp/data/fields/text_field.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, vocab)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mtoken_index_to_indexer_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindexer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_indexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mtoken_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_to_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mtoken_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mindexer_name_to_indexed_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/allennlp/allennlp/data/token_indexers/elmo_indexer.py\u001b[0m in \u001b[0;36mtokens_to_indices\u001b[0;34m(self, tokens, vocabulary, index_name)\u001b[0m\n\u001b[1;32m    129\u001b[0m             raise ConfigurationError('ELMoTokenCharactersIndexer needs a tokenizer '\n\u001b[1;32m    130\u001b[0m                                      'that retains text')\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_word_to_char_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/allennlp/allennlp/data/token_indexers/elmo_indexer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m             raise ConfigurationError('ELMoTokenCharactersIndexer needs a tokenizer '\n\u001b[1;32m    130\u001b[0m                                      'that retains text')\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_word_to_char_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/allennlp/allennlp/data/token_indexers/elmo_indexer.py\u001b[0m in \u001b[0;36mconvert_word_to_char_ids\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_word_to_char_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_to_add\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mchar_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mELMoCharacterMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_character\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mELMoCharacterMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_word_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mchar_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mELMoCharacterMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeginning_of_word_character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()\n",
    "tokens = [[\"I\", \"ate\", \"an\", \"apple\", \"for\", \"breakfast\"],[\"i\", \"tired\"]]\n",
    "vectors = elmo.embed_sentence(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6.9227195e-01, -3.2613140e-01,  2.2827481e-01, ...,\n",
       "          1.7574832e-01,  2.6598692e-01, -1.0131966e-01],\n",
       "        [-6.6311532e-01,  2.9512239e-01,  6.5207249e-01, ...,\n",
       "          6.5627509e-01,  4.2394829e-01,  1.2068850e+00],\n",
       "        [ 7.9657219e-02,  1.9919696e-01, -6.9490090e-02, ...,\n",
       "          2.1669518e-02,  1.2296101e-01,  4.1096085e-03],\n",
       "        [ 1.4436819e-01,  6.7775212e-02,  3.7736291e-01, ...,\n",
       "          4.1031605e-01,  2.9029369e-01, -6.1045110e-02],\n",
       "        [-2.4150285e-01,  5.4133460e-02, -3.1142169e-01, ...,\n",
       "          4.0102765e-01, -2.6212469e-01, -4.2983261e-01],\n",
       "        [-2.3136683e-01,  2.7052867e-01,  4.4679993e-01, ...,\n",
       "          2.0299812e-01, -2.5246483e-01,  4.1547716e-02]],\n",
       "\n",
       "       [[-1.1051465e+00, -4.0921757e-01, -4.3645081e-01, ...,\n",
       "         -5.5361629e-01, -2.2313195e-01,  3.2954097e-02],\n",
       "        [ 2.3847309e-01, -2.4085942e-01,  2.2867616e-02, ...,\n",
       "          3.5999551e-01, -1.5432365e-02,  9.6235320e-02],\n",
       "        [ 6.7190900e-02, -1.4298743e-01, -3.6074755e-01, ...,\n",
       "          3.6400229e-01, -1.1033118e-03,  7.2196734e-01],\n",
       "        [ 7.5014079e-01, -7.0626223e-01,  3.9626992e-01, ...,\n",
       "         -3.5477236e-01,  5.3506887e-01,  1.1609949e+00],\n",
       "        [-1.9890222e-01, -3.6067361e-01,  2.7924347e-01, ...,\n",
       "          2.5100589e-02, -3.9890850e-01,  5.1120037e-01],\n",
       "        [-5.1222438e-01, -4.7477061e-01, -2.0949888e-01, ...,\n",
       "         -6.3179880e-03, -3.5467827e-01,  6.5792096e-01]],\n",
       "\n",
       "       [[-3.2634320e+00, -9.4477582e-01, -3.1986564e-01, ...,\n",
       "         -1.6834047e+00,  2.3847967e-02,  3.8188225e-01],\n",
       "        [-7.4718213e-01, -4.5465246e-01,  1.4126738e+00, ...,\n",
       "          3.4490243e-01,  3.6524558e-01,  6.6159040e-01],\n",
       "        [-2.3929690e-01, -1.1358550e+00, -1.0007650e-02, ...,\n",
       "         -1.9812220e-01, -1.0775578e+00,  9.9873799e-01],\n",
       "        [ 1.2281965e+00, -5.1431459e-01,  1.6688713e+00, ...,\n",
       "         -5.9785104e-01, -3.6315382e-02,  2.2921367e+00],\n",
       "        [-4.1551197e-01, -7.9405320e-01,  6.0101932e-01, ...,\n",
       "         -7.2129881e-01, -6.3341421e-01,  1.3248782e+00],\n",
       "        [-3.5665435e-01, -5.5098534e-01,  1.3024213e+00, ...,\n",
       "         -3.9886665e-01, -6.4261854e-01,  6.5832150e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
